{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "## python libs\n",
    "import os\n",
    "import numpy as np\n",
    "## tf-Keras libs\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dropout, Concatenate\n",
    "#from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.layers import BatchNormalization, Activation, MaxPooling2D\n",
    "from keras.applications import vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG19_Content(dataset='imagenet'):\n",
    "    # Load VGG, trained on imagenet data\n",
    "    vgg = vgg19.VGG19(include_top=False, weights=dataset)\n",
    "    vgg.trainable = False\n",
    "    content_layers = ['block5_conv2']\n",
    "    content_outputs = [vgg.get_layer(name).output for name in content_layers]\n",
    "    return Model(vgg.input, content_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FUNIE_GAN():\n",
    "    def __init__(self, imrow=256, imcol=256, imchan=3, loss_meth='wgan'):\n",
    "        ## input image shape\n",
    "        self.img_rows, self.img_cols, self.channels = imrow, imcol, imchan\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        ## input images and their conditioning images\n",
    "        img_A = Input(shape=self.img_shape)\n",
    "        img_B = Input(shape=self.img_shape)\n",
    "        ## conv 5_2 content from vgg19 network\n",
    "        self.vgg_content = VGG19_Content()\n",
    "        ## output shape of D (patchGAN)\n",
    "        self.disc_patch = (16, 16, 1)\n",
    "        ## number of filters in the first layer of G and D\n",
    "        self.gf, self.df = 32, 32\n",
    "        optimizer = Adam(0.0003, 0.5)\n",
    "        ## Build and compile the discriminator\n",
    "        self.discriminator = self.FUNIE_discriminator()\n",
    "        self.discriminator.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
    "        ## Build the generator\n",
    "        self.generator = self.FUNIE_generator2()\n",
    "        ## By conditioning on B generate a fake version of A\n",
    "        fake_A = self.generator(img_B)\n",
    "        ## For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "        ## Discriminators determines validity of translated images / condition pairs\n",
    "        valid = self.discriminator([fake_A, img_B])\n",
    "        ## compute the comboned loss\n",
    "        self.combined = Model(inputs=[img_A, img_B], outputs=[valid, fake_A])\n",
    "        self.combined.compile(loss=['mse', self.total_gen_loss], loss_weights=[0.2, 0.8], optimizer=optimizer)\n",
    "        \n",
    "    def wasserstein_loss(self, y_true, y_pred):\n",
    "        # for wasserstein GAN loss\n",
    "        return K.mean(y_true * y_pred)\n",
    "    \n",
    "\n",
    "    def perceptual_distance(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "           Calculating perceptual distance\n",
    "           Thanks to github.com/wandb/superres\n",
    "        \"\"\"\n",
    "        y_true = (y_true+1.0)*127.5 # [-1,1] -> [0, 255]\n",
    "        y_pred = (y_pred+1.0)*127.5 # [-1,1] -> [0, 255]\n",
    "        rmean = (y_true[:, :, :, 0] + y_pred[:, :, :, 0]) / 2\n",
    "        r = y_true[:, :, :, 0] - y_pred[:, :, :, 0]\n",
    "        g = y_true[:, :, :, 1] - y_pred[:, :, :, 1]\n",
    "        b = y_true[:, :, :, 2] - y_pred[:, :, :, 2]\n",
    "        return K.mean(K.sqrt((((512+rmean)*r*r)/256) + 4*g*g + (((767-rmean)*b*b)/256)))\n",
    "\n",
    "\n",
    "    def total_gen_loss(self, org_content, gen_content):\n",
    "        # custom perceptual loss function\n",
    "        vgg_org_content = self.vgg_content(org_content)\n",
    "        vgg_gen_content = self.vgg_content(gen_content)\n",
    "        content_loss = K.mean(K.square(vgg_org_content - vgg_gen_content), axis=-1)\n",
    "        mae_gen_loss = K.mean(K.abs(org_content-gen_content))\n",
    "        perceptual_loss = self.perceptual_distance(org_content, gen_content)\n",
    "        gen_total_err = 0.7*mae_gen_loss+0.3*content_loss # v1\n",
    "        # updated loss function in v2\n",
    "        #gen_total_err = 0.6*mae_gen_loss+0.3*content_loss+0.1*perceptual_loss\n",
    "        return gen_total_err\n",
    "\n",
    "\n",
    "    def FUNIE_generator1(self):\n",
    "        \"\"\"\n",
    "           Inspired by the U-Net Generator with skip connections\n",
    "           This is a much simpler architecture with fewer parameters (faster inference)\n",
    "        \"\"\"\n",
    "        def conv2d(layer_input, filters, f_size=3, bn=True):\n",
    "            ## for downsampling\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            #d = LeakyReLU(alpha=0.2)(d)\n",
    "            d = Activation('relu')(d)\n",
    "            if bn: d = BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "\n",
    "        def deconv2d(layer_input, skip_input, filters, f_size=3, dropout_rate=0):\n",
    "            ## for upsampling\n",
    "            u = UpSampling2D(size=2)(layer_input)\n",
    "            u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "            if dropout_rate: u = Dropout(dropout_rate)(u)\n",
    "            u = BatchNormalization(momentum=0.8)(u)\n",
    "            u = Concatenate()([u, skip_input])\n",
    "            return u\n",
    "        ## input\n",
    "        d0 = Input(shape=self.img_shape); print(d0)\n",
    "        ## downsample\n",
    "        d1 = conv2d(d0, self.gf*1, f_size=5, bn=False)\n",
    "        d2 = conv2d(d1, self.gf*4, f_size=4, bn=True)\n",
    "        d3 = conv2d(d2, self.gf*8, f_size=4, bn=True)\n",
    "        d4 = conv2d(d3, self.gf*8, f_size=3, bn=True)\n",
    "        d5 = conv2d(d4, self.gf*8, f_size=3, bn=True)\n",
    "        ## upsample\n",
    "        u1 = deconv2d(d5, d4, self.gf*8)\n",
    "        u2 = deconv2d(u1, d3, self.gf*8)\n",
    "        u3 = deconv2d(u2, d2, self.gf*4)\n",
    "        u4 = deconv2d(u3, d1, self.gf*1)\n",
    "        u5 = UpSampling2D(size=2)(u4)\n",
    "        output_img = Conv2D(self.channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u5)\n",
    "        print(output_img); print();\n",
    "\n",
    "        return Model(d0, output_img)\n",
    "\n",
    "\n",
    "    def FUNIE_generator2(self):\n",
    "        \"\"\"\n",
    "           Inspired by the U-Net Generator with skip connections\n",
    "           This is a much simpler architecture with fewer parameters (faster inference)\n",
    "        \"\"\"\n",
    "        def conv2d(layer_input, filters, f_size=3, bn=True):\n",
    "            ## for downsampling\n",
    "            d = Conv2D(filters, kernel_size=f_size, padding='same')(layer_input)\n",
    "            #d = LeakyReLU(alpha=0.2)(d)\n",
    "            d = Activation('relu')(d)\n",
    "            if bn: d = BatchNormalization(momentum=0.75)(d)\n",
    "            return d\n",
    "\n",
    "        def deconv2d(layer_input, skip_input, filters, f_size=3, dropout_rate=0):\n",
    "            ## for upsampling\n",
    "            u = UpSampling2D(size=2)(layer_input)\n",
    "            u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "            if dropout_rate: u = Dropout(dropout_rate)(u)\n",
    "            u = BatchNormalization(momentum=0.8)(u)\n",
    "            u = Concatenate()([u, skip_input])\n",
    "            return u\n",
    "        ## input\n",
    "        d0 = Input(shape=self.img_shape); print(d0)\n",
    "        ## downsample\n",
    "        d1 = conv2d(d0, self.gf*1, f_size=5, bn=False)\n",
    "        d1a = MaxPooling2D(pool_size=(2, 2))(d1)\n",
    "        d2 = conv2d(d1a, self.gf*2, f_size=4, bn=True)\n",
    "        d3 = conv2d(d2, self.gf*2, f_size=4, bn=True)\n",
    "        d3a = MaxPooling2D(pool_size=(2, 2))(d3)\n",
    "        d4 = conv2d(d3a, self.gf*4, f_size=3, bn=True)\n",
    "        d5 = conv2d(d4, self.gf*4, f_size=3, bn=True)\n",
    "        d5a = MaxPooling2D(pool_size=(2, 2))(d5)\n",
    "        d6 = conv2d(d5a, self.gf*8, f_size=3, bn=True)\n",
    "        ## upsample\n",
    "        u1 = deconv2d(d6, d5, self.gf*8)\n",
    "        u2 = deconv2d(u1, d3, self.gf*8)\n",
    "        u3 = deconv2d(u2, d1, self.gf*4)\n",
    "        u4 = conv2d(u3, self.gf*4, f_size=3)\n",
    "        u5 = conv2d(u4, self.gf*8, f_size=3)\n",
    "        output_img = Conv2D(self.channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u5)\n",
    "        return Model(d0, output_img)\n",
    "\n",
    "\n",
    "\n",
    "    def FUNIE_discriminator(self):\n",
    "        \"\"\"\n",
    "           Inspired by the pix2pix discriminator\n",
    "        \"\"\"\n",
    "        def d_layer(layer_input, filters, strides_=2,f_size=3, bn=True):\n",
    "            ## Discriminator layers\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=strides_, padding='same')(layer_input)\n",
    "            #d = LeakyReLU(alpha=0.2)(d)\n",
    "            d = Activation('relu')(d)\n",
    "            if bn: d = BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "\n",
    "        img_A = Input(shape=self.img_shape)\n",
    "        img_B = Input(shape=self.img_shape)\n",
    "        ## input\n",
    "        combined_imgs = Concatenate(axis=-1)([img_A, img_B])\n",
    "        ## Discriminator layers\n",
    "        d1 = d_layer(combined_imgs, self.df, bn=False)\n",
    "        d2 = d_layer(d1, self.df*2)\n",
    "        d3 = d_layer(d2, self.df*4) \n",
    "        d4 = d_layer(d3, self.df*8)\n",
    "        validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
    "        # return model\n",
    "        return Model([img_A, img_B], validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# > Various modules for handling data \n",
    "# > Maintainer: https://github.com/xahidbuffon\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "from scipy import misc\n",
    "\n",
    "def deprocess(x):\n",
    "    # [-1,1] -> [0, 255]\n",
    "    return (x+1.0)*127.5\n",
    "\n",
    "def preprocess(x):\n",
    "    # [0,255] -> [-1, 1]\n",
    "    return (x/127.5)-1.0\n",
    "\n",
    "def augment(a_img, b_img):\n",
    "    \"\"\"\n",
    "       Augment images - a is distorted\n",
    "    \"\"\"\n",
    "    # randomly interpolate\n",
    "    a = random.random()\n",
    "    a_img = a_img*(1-a) + b_img*a\n",
    "    # flip image left right\n",
    "    if (random.random() < 0.25):\n",
    "        a_img = np.fliplr(a_img)\n",
    "        b_img = np.fliplr(b_img)\n",
    "    # flip image up down\n",
    "    if (random.random() < 0.25):\n",
    "        a_img = np.flipud(a_img)\n",
    "        b_img = np.flipud(b_img) \n",
    "    return a_img, b_img\n",
    "\n",
    "def getPaths(data_dir):\n",
    "    exts = ['*.png','*.PNG','*.jpg','*.JPG', '*.JPEG']\n",
    "    image_paths = []\n",
    "    for pattern in exts:\n",
    "        for d, s, fList in os.walk(data_dir):\n",
    "            for filename in fList:\n",
    "                if (fnmatch.fnmatch(filename, pattern)):\n",
    "                    fname_ = os.path.join(d,filename)\n",
    "                    image_paths.append(fname_)\n",
    "    return np.asarray(image_paths)\n",
    "\n",
    "def read_and_resize(path, img_res):\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float)\n",
    "    img = cv2.resize(img, img_res)\n",
    "    return img\n",
    "\n",
    "def read_and_resize_pair(pathA, pathB, img_res): \n",
    "    img_A = cv2.imread(pathA)\n",
    "    img_A = cv2.cvtColor(img_A, cv2.COLOR_BGR2RGB).astype(np.float)\n",
    "    img_A = cv2.resize(img_A, img_res)\n",
    "    img_B = cv2.imread(pathB)\n",
    "    img_B = cv2.cvtColor(img_B, cv2.COLOR_BGR2RGB).astype(np.float)\n",
    "    img_B = cv2.resize(img_B, img_res)\n",
    "    return img_A, img_B\n",
    "\n",
    "def get_local_test_data(data_dir, img_res=(256, 256)):\n",
    "    assert os.path.exists(data_dir), \"local image path doesnt exist\"\n",
    "    imgs = []\n",
    "    for p in getPaths(data_dir):\n",
    "        img = read_and_resize(p, img_res)\n",
    "        imgs.append(img)\n",
    "    imgs = preprocess(np.array(imgs))\n",
    "    return imgs\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, data_dir, dataset_name, img_res=(256, 256), test_only=False):\n",
    "        self.img_res = img_res\n",
    "        self.DATA = dataset_name\n",
    "        self.data_dir = data_dir\n",
    "        if not test_only:\n",
    "            self.trainA_paths = getPaths(os.path.join(self.data_dir, \"trainA\")) # distorted\n",
    "            self.trainB_paths = getPaths(os.path.join(self.data_dir, \"trainB\")) # enhanced\n",
    "            if (len(self.trainA_paths)<len(self.trainB_paths)):\n",
    "                self.trainB_paths = self.trainB_paths[:len(self.trainA_paths)]\n",
    "            elif (len(self.trainA_paths)>len(self.trainB_paths)):\n",
    "                self.trainA_paths = self.trainA_paths[:len(self.trainB_paths)]\n",
    "            else: pass\n",
    "            self.val_paths = getPaths(os.path.join(self.data_dir, \"validation\"))\n",
    "            self.num_train, self.num_val = len(self.trainA_paths), len(self.val_paths)\n",
    "            print (\"{0} training pairs\\n\".format(self.num_train))\n",
    "        else:\n",
    "            self.test_paths    = getPaths(os.path.join(self.data_dir, \"test\"))\n",
    "            print (\"{0} test images\\n\".format(len(self.test_paths)))\n",
    "\n",
    "    def get_test_data(self, batch_size=1):\n",
    "        idx = np.random.choice(np.arange(len(self.test_paths)), batch_size, replace=False)\n",
    "        paths = self.test_paths[idx]\n",
    "        imgs = []\n",
    "        for p in paths:\n",
    "            img = read_and_resize(p, self.img_res)\n",
    "            imgs.append(img)\n",
    "        imgs = preprocess(np.array(imgs))\n",
    "        return imgs\n",
    "\n",
    "    def load_val_data(self, batch_size=1):\n",
    "        idx = np.random.choice(np.arange(self.num_val), batch_size, replace=False)\n",
    "        pathsA = self.trainA_paths[idx]\n",
    "        pathsB = self.trainB_paths[idx]\n",
    "        imgs_A, imgs_B = [], []\n",
    "        for idx in range(len(pathsB)):\n",
    "            img_A, img_B = read_and_resize_pair(pathsA[idx], pathsB[idx], self.img_res)\n",
    "            imgs_A.append(img_A)\n",
    "            imgs_B.append(img_B)\n",
    "        imgs_A = preprocess(np.array(imgs_A))\n",
    "        imgs_B = preprocess(np.array(imgs_B))\n",
    "        return imgs_A, imgs_B\n",
    "\n",
    "    def load_batch(self, batch_size=1, data_augment=True):\n",
    "        self.n_batches = self.num_train//batch_size\n",
    "        for i in range(self.n_batches-1):\n",
    "            batch_A = self.trainA_paths[i*batch_size:(i+1)*batch_size]\n",
    "            batch_B = self.trainB_paths[i*batch_size:(i+1)*batch_size]\n",
    "            imgs_A, imgs_B = [], []\n",
    "            for idx in range(len(batch_A)): \n",
    "                img_A, img_B = read_and_resize_pair(batch_A[idx], batch_B[idx], self.img_res)\n",
    "                if (data_augment):\n",
    "                    img_A, img_B = augment(img_A, img_B)\n",
    "                imgs_A.append(img_A)\n",
    "                imgs_B.append(img_B)\n",
    "            imgs_A = preprocess(np.array(imgs_A))\n",
    "            imgs_B = preprocess(np.array(imgs_B))\n",
    "            yield imgs_A, imgs_B\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../input/evprdata/Paired/\"\n",
    "dataset_name = \"underwater_imagenet\" # options: {'underwater_imagenet', 'underwater_dark'}\n",
    "data_loader = DataLoader(os.path.join(data_dir, dataset_name), dataset_name)\n",
    "## create dir for log and (sampled) validation data\n",
    "samples_dir = os.path.join(\"./samples/funieGAN/\", dataset_name)\n",
    "checkpoint_dir = os.path.join(\"./checkpoints/funieGAN/\", dataset_name)\n",
    "if not os.path.exists(samples_dir): os.makedirs(samples_dir)\n",
    "if not os.path.exists(checkpoint_dir): os.makedirs(checkpoint_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 7\n",
    "batch_size = 4\n",
    "val_interval = 200\n",
    "N_val_samples = 3\n",
    "save_model_interval = data_loader.num_train//batch_size\n",
    "num_step = num_epoch*save_model_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_interval, data_loader.num_train, num_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load model arch\n",
    "funie_gan = FUNIE_GAN()\n",
    "## ground-truths for adversarial loss\n",
    "valid = np.ones((batch_size,) + funie_gan.disc_patch)\n",
    "fake = np.zeros((batch_size,) + funie_gan.disc_patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "from scipy import misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_val_samples_funieGAN(samples_dir, gen_imgs, step, N_samples=3, N_ims=3):\n",
    "    row=N_samples; col=N_ims;\n",
    "    titles = ['Input', 'Generated', 'Original']\n",
    "    fig, axs = plt.subplots(row, col)\n",
    "    cnt = 0\n",
    "    for j in range(col):\n",
    "        for i in range(row): \n",
    "            axs[i,j].imshow(gen_imgs[cnt])\n",
    "            axs[i, j].set_title(titles[j])\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(os.path.join(samples_dir, (\"%d.png\" %step)))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def save_val_samples_unpaired(samples_dir, gen_imgs, step, N_samples=1, N_ims=6):\n",
    "    row=2*N_samples; col=N_ims//2;\n",
    "    titles = ['Original','Translated','Reconstructed']\n",
    "    fig, axs = plt.subplots(row, col)\n",
    "    cnt = 0\n",
    "    for i in range(row):\n",
    "        for j in range(col): \n",
    "            axs[i,j].imshow(gen_imgs[cnt])\n",
    "            axs[i, j].set_title(titles[j])\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(os.path.join(samples_dir, (\"_%d.png\" %step)))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def save_test_samples_funieGAN(samples_dir, gen_imgs, step=0):\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    axs[0].imshow(gen_imgs[0])\n",
    "    axs[0].set_title(\"Input\")\n",
    "    axs[0].axis('off')\n",
    "    axs[1].imshow(gen_imgs[1])\n",
    "    axs[1].set_title(\"Generated\")\n",
    "    axs[1].axis('off')\n",
    "    fig.savefig(os.path.join(samples_dir,(\"_test_%d.png\" %step)))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def viz_gen_and_dis_losses(all_D_losses, all_G_losses, save_dir=None):\n",
    "    plt.plot(all_D_losses, 'r')\n",
    "    plt.plot(all_G_losses, 'g')\n",
    "    plt.title('Model convergence'); plt.ylabel('Losses'); plt.xlabel('# of steps');\n",
    "    plt.legend(['Discriminator network', 'Generator network'], loc='upper right')\n",
    "    plt.show();\n",
    "    if not save_dir:\n",
    "        plt.savefig(os.path.join(save_dir, '_conv.png'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training loop\n",
    "step = 0\n",
    "all_D_losses = []; all_G_losses = []\n",
    "while (step <= num_step):\n",
    "    for _, (imgs_distorted, imgs_good) in enumerate(data_loader.load_batch(batch_size)):\n",
    "        ##  train the discriminator\n",
    "        imgs_fake = funie_gan.generator.predict(imgs_distorted)\n",
    "        d_loss_real = funie_gan.discriminator.train_on_batch([imgs_good, imgs_distorted], valid)\n",
    "        d_loss_fake = funie_gan.discriminator.train_on_batch([imgs_fake, imgs_distorted], fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        ## train the generator\n",
    "        g_loss = funie_gan.combined.train_on_batch([imgs_good, imgs_distorted], [valid, imgs_good])\n",
    "        ## increment step, save losses, and print them \n",
    "        step += 1; all_D_losses.append(d_loss[0]);  all_G_losses.append(g_loss[0]);\n",
    "        if step%50==0:\n",
    "            print (\"Step {0}/{1}: lossD: {2}, lossG: {3}\".format(step, num_step, d_loss[0], g_loss[0])) \n",
    "        ## validate and save generated samples at regular intervals \n",
    "        if (step % val_interval==0):\n",
    "            imgs_distorted, imgs_good = data_loader.load_val_data(batch_size=N_val_samples)\n",
    "            imgs_fake = funie_gan.generator.predict(imgs_distorted)\n",
    "            gen_imgs = np.concatenate([imgs_distorted, imgs_fake, imgs_good])\n",
    "            gen_imgs = 0.5 * gen_imgs + 0.5 # Rescale to 0-1\n",
    "            save_val_samples_funieGAN(samples_dir, gen_imgs, step, N_samples=N_val_samples)\n",
    "            titles = ['Input', 'Generated', 'Original']\n",
    "            fig, axs = plt.subplots(3, 3)\n",
    "            cnt = 0\n",
    "            for j in range(3):\n",
    "                for i in range(N_val_samples): \n",
    "                    axs[i,j].imshow(gen_imgs[cnt])\n",
    "                    axs[i, j].set_title(titles[j])\n",
    "                    axs[i,j].axis('off')\n",
    "                    cnt += 1\n",
    "            plt.show()\n",
    "        ## save model and weights\n",
    "        if (step % save_model_interval==0):\n",
    "            model_name = os.path.join(checkpoint_dir, (\"model_%d\" %step))\n",
    "            with open(model_name+\"_.json\", \"w\") as json_file:\n",
    "                json_file.write(funie_gan.generator.to_json())\n",
    "            funie_gan.generator.save_weights(model_name+\"_.h5\")\n",
    "            print(\"\\nSaved trained model in {0}\\n\".format(checkpoint_dir))\n",
    "        ## sanity\n",
    "        if (step>=num_step): break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].imshow(gen_imgs[2])\n",
    "axs[0].set_title(\"Input\")\n",
    "axs[0].axis('off')\n",
    "axs[1].imshow(gen_imgs[2])\n",
    "axs[1].set_title(\"Generated\")\n",
    "axs[1].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row=3\n",
    "col=3\n",
    "fig, axs = plt.subplots(row, col)\n",
    "cnt = 0\n",
    "for j in range(col):\n",
    "    for i in range(row): \n",
    "        axs[i,j].imshow(gen_imgs[cnt])\n",
    "        axs[i,j].axis('off')\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
